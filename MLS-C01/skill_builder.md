# Exam Readiness: AWS Certified Machine Learning - Specialty
- ![image](https://github.com/mjs1995/Certified-AWS/assets/47103479/cacabb8b-4534-4b07-b95b-06ce3c876a80)
- 데이터 엔지니어링
  - AWS Lake Formation 및 Amazon S3
    - AWS Lake Formation은 데이터 레이크 솔루션이며, Amazon S3는 AWS에서의 데이터 과학 처리용으로 많이 사용되는 스토리지 옵션입니다.
  - Amazon S3와 Amazon SageMaker
    - Amazon SageMaker를 사용하여 기계 학습 모형을 훈련하는 동안 Amazon S3를 사용할 수 있습니다. Amazon S3는 Amazon SageMaker와 통합되어 훈련 데이터와 모형 훈련 결과를 저장합니다.
  - Amazon FSx for Lustre
    - 훈련 데이터가 이미 Amazon S3에 있고 다른 알고리즘과 파라미터를 사용하여 훈련 작업을 여러 번 실행할 계획이라면 파일 시스템 서비스인 Amazon FSx for Lustre를 사용하는 것이 좋습니다. 
    - FSx for Lustre는 Amazon SageMaker에 Amazon S3 데이터를 제공하여 훈련 작업을 가속화합니다. 
    - 훈련 작업을 처음 실행할 때 FSx for Lustre는 Amazon S3의 데이터를 자동으로 복사하여 Amazon SageMaker에서 사용할 수 있도록 합니다. 
    - 훈련 작업의 후속 반복에 동일한 Amazon FSx 파일 시스템을 사용함으로써 공통 Amazon S3 객체를 반복해서 다운로드하는 것을 방지할 수 있습니다.
  - Amazon S3와 Amazon EFS
    - 훈련 데이터가 이미 Amazon Elastic File System(EFS)에 있는 경우 이를 훈련 데이터 소스로 사용하는 것이 좋습니다. 
    - Amazon EFS는 데이터 이동 없이 서비스에서 훈련 작업을 바로 시작할 수 있으므로 더 빨리 훈련을 시작할 수 있습니다. 데이터 과학자가 Amazon EFS에 홈 디렉터리를 가지고 있어서 새로운 데이터를 가져오고 동료와 데이터를 공유하고 데이터 집합에 다양한 필드 또는 레이블을 추가하는 실험을 통해 모형을 빠르게 반복하는 환경에서는 흔히 있는 일입니다
  - 데이터 수집의 두 가지 유형인 배치 처리와 스트림 처리
    - 배치 처리
      - 배치 처리에서는 주기적으로 소스 데이터를 수집하고 그룹화함
        - 배치 처리를 통해 수집 계층은 정기적으로 소스 데이터를 수집 및 그룹화하여 Amazon S3와 같은 대상으로 전송합니다. 
        - 논리적 순서, 특정 조건의 활성화 또는 간단한 일정에 따라 그룹을 처리할 수 있습니다. 
        - 배치 처리는 일반적으로 다른 수집 옵션보다 경제적으로 쉽게 구현되기 때문에 실시간 또는 거의 실시간에 가까운 데이터가 필요하지 않은 경우에 사용됩니다.
      - AWS 클라우드로의 배치 처리를 지원하는 여러 서비스
        - AWS 클라우드로의 배치 수집에는 데이터를 분류, 정리 및 보강하고 다양한 데이터 스토어 간에 이동하는 데 사용할 수 있는 ETL(추출, 변환 및 로드) 서비스인 AWS Glue와 같은 서비스를 이용할 수 있습니다. 
        - AWS Database Migration Service(AWS DMS)는 배치 수집을 지원하는 또 다른 서비스입니다. 이 서비스는 지정한 간격으로 관계형 데이터베이스 관리 시스템, 데이터 웨어하우스 및 NoSQL 데이터베이스와 같은 소스 시스템에서 기록 데이터를 읽습니다. 
        - AWS Step Functions를 사용하여 복잡한 워크플로와 관련된 다양한 ETL 작업을 자동화할 수 있습니다.
    - 스트림 처리
      - 스트림 처리에서는 인식되는 대로 데이터 조작 및 로드
        - 실시간 처리를 포함하는 스트림 처리에는 그룹화가 전혀 포함되지 않습니다. 
        - 데이터가 생성되거나 데이터 수집 계층에서 인식되는 즉시 소싱되고 조작되고 로드됩니다. 이러한 종류의 수집은 시스템이 소스를 지속적으로 모니터링하고 새로운 정보를 수용해야 하므로 경제성이 떨어집니다. 
        - Amazon SageMaker 엔드포인트를 사용하여 웹 사이트에서 고객에게 표시하려는 실시간 예측이나, 실시간 대시보드와 같이 지속적으로 새로 고쳐지는 데이터가 필요한 실시간 분석에 사용하려 할 수 있습니다.
      - Amazon Kinesis는 AWS에서 데이터를 스트리밍하기 위한 플랫폼
        - AWS에서 데이터를 스트리밍하기 위한 플랫폼인 Amazon Kinesis를 사용하여 빠르게 이동하는 데이터를 캡처하고 수집하는 것이 효과적입니다. 
        - Amazon Kinesis는 특수한 요구 사항에 맞는 맞춤형 스트리밍 데이터 애플리케이션을 구축할 수 있는 기회를 제공하며, 스트리밍 데이터를 보다 쉽게 로드하고 분석하는 데 중점을 둔 여러 서비스를 제공합니다.
  - 데이터 변환 솔루션 식별 및 구현
    - 수집된 원시 데이터는 기계 학습에 사용할 수 없음
      - 데이터 중복 제거, 불완전한 데이터 관리 및 속성 표준화를 비롯한 데이터 변환 및 정리 단계를 거쳐야 합니다. 
      - 데이터 변환에서는 필요에 따라 데이터의 쿼리를 용이하게 하기 위해 보통 OLAP 모형으로 데이터 구조를 변경할 수 있습니다. 
    - 기계 학습을 위한 데이터 변환
      - MapReduce 및 Apache Spark와 같은 분산 컴퓨팅 프레임워크는 데이터 처리와 노드 작업 배포 및 관리를 위한 프로토콜을 제공합니다. 
      - 알고리즘을 사용하여 데이터 집합을 하위 집합으로 분할하고 컴퓨팅 클러스터의 노드에 분산합니다.
    - Amazon EMR에서 Apache Spark를 사용하여 관리형 프레임워크 구축
      - Amazon EMR에서 Apache Spark를 사용하면 방대한 양의 데이터를 처리할 수 있는 관리형 프레임워크가 구축됩니다. 
      - Amazon EMR은 HPC(고성능 컴퓨팅) 애플리케이션에 적합하도록 향상된 네트워크 성능과 그에 비례하는 높은 CPU 성능을 제공하는 다양한 인스턴스 유형을 지원합니다.
    - 기계 학습을 위한 데이터 변환의 핵심 단계는 데이터 집합을 분할하는 것입니다.
      - 기계 학습 애플리케이션에 필요한 데이터 집합은 데이터베이스 웨어하우스, 스트리밍 IoT 입력 또는 중앙 집중식 데이터 레이크에서 가져옵니다
      - ETL 처리 서비스(Amazon Athena, AWS Glue, Amazon Redshift Spectrum)는 기능을 보완하며, Amazon S3에 저장되거나 Amazon S3를 대상으로 하는 데이터 집합을 사전 처리하도록 구축할 수 있습니다
      - Athena 및 Amazon Redshift Spectrum 같은 서비스로 데이터를 변환하는 것 외에 AWS Glue와 같은 서비스를 사용하여 메타데이터 검색 및 관리 기능을 제공할 수도 있습니다
    - Amazon S3에 단일 데이터 소스를 저장하고 임시 분석을 수행할 수 있습니다.
      - 고객은 Amazon S3에 단일 데이터 소스를 저장하고 Athena를 사용하여 임시 분석을 수행하고, Amazon Redshift의 데이터 웨어하우스와 통합하고, Amazon QuickSight를 사용하여 지표를 보여주는 시각적 대시보드를 구축하며, Amazon SageMaker를 사용하여 재입원을 예측하는 기계 학습 모형을 구축할 수 있습니다. 데이터를 이동하지 않고 다양한 서비스를 사용하여 데이터에 연결함으로써 고객은 동일한 데이터의 중복 복사본을 만들지 않아도 됩니다.
      - ![image](https://github.com/mjs1995/Certified-AWS/assets/47103479/2a8326f8-5bc7-4be6-b26a-56b8b2185b73)
- 탐색적 데이터 분석
  - 일반적으로 사용되는 척도화 및 정규화 변환 모음
    - 평균/분산 표준화
    - MinMax 척도화
    - Maxabs 척도화
    - 로버스트 척도화
    - 정규화
  - 시각화를 통해 특정 특성을 더 잘 파악
    - 데이터의 범위는 어떻게 됩니까?
    - 데이터의 최고점은 얼마입니까?
    - 특이치가 있습니까?
    - 데이터에 흥미로운 패턴이 있습니까?
- 모형화
  - NLP 및 CV를 위한 Amazon SageMaker 기본 제공 알고리즘
    - NLP(자연어 처리)를 위한 알고리즘
      - 자연어 처리를 위한 Amazon SageMaker의 기본 제공 알고리즘이 있습니다.
      - BlazingText 알고리즘은 Word2Vec 및 텍스트 분류 알고리즘의 고도로 최적화된 구현을 제공합니다.
      - Sequence2Sequence는 일련의 토큰(예: 텍스트, 오디오)을 입력으로 사용하고 또 다른 토큰 시퀀스를 출력으로 생성하는 지도 학습 알고리즘입니다.
      - Object2Vec은 Amazon SageMaker BlazingText 알고리즘에 최적화된 단어를 임베딩하는 유명한 Word2Vec 기법을 일반화합니다.
  - CV(컴퓨터 비전)을 위한 알고리즘
    - 컴퓨터 비전을 위한 Amazon SageMaker 기본 제공 알고리즘이 있습니다.
      - 이미지 분류는 이미지를 분류하는 데 사용되는 지도 학습 알고리즘입니다.
      - 물체 감지 알고리즘은 단일 심층 신경망을 사용하여 이미지의 물체를 감지하고 분류합니다. 이 알고리즘은 입력으로 이미지를 가져와 이미지 장면 내에서 객체의 모든 인스턴스를 식별하는 지도 학습 알고리즘입니다. 객체는 지정된 모음 내 클래스 중 하나로 범주화되는데, 이때 해당 클래스에 속하는 신뢰도 점수를 사용합니다. 이미지에서 해당 위치와 척도는 사각형 경계 상자로 표시됩니다.
      - 의미 체계 세분화 알고리즘은 미리 정의된 클래스 집합의 클래스 레이블로 이미지의 모든 픽셀에 태깅합니다.
  - 기타 훈련 알고리즘 옵션
    - Amazon SageMaker에서 Apache Spark 사용
    - 사용자 지정 코드를 제출하여 TensorFlow 또는 Apache MXNet 같은 딥 러닝 프레임워크로 모형 훈련
    - 자체 사용자 지정 알고리즘을 사용하고 코드를 Docker 이미지에 포함
    - AWS Marketplace에서 알고리즘 구독
  - 하이퍼 파라미터
    - 하이퍼파라미터는 훈련 작업을 실행하기 전에 튜닝하여 기계 학습 알고리즘의 동작을 제어할 수 있는 노브 또는 설정입니다. 하이퍼파라미터는 훈련 시간, 모형 수렴 및 모형 정확도와 관련이 있으므로 모형 훈련에 큰 영향을 미칠 수 있습니다. 훈련 작업에서 파생되는 모형 파라미터와 달리 하이퍼파라미터 값은 훈련 중에 변경되지 않습니다.
      - 모형 하이퍼파라미터
        - 모형 자체(필터 크기, 풀링, 스트라이드, 패딩과 같은 신경망 아키텍처의 속성)를 정의합니다.
      - 옵티마이저 하이퍼파라미터
        - 모형이 데이터를 기반으로 패턴을 학습하는 방법과 관련된 유형으로, 신경망 모형에 사용됩니다.
        - 경사 하강법 및 확률적 경사 하강법 같은 옵티마이저, Adam과 같은 모멘텀을 사용하는 옵티마이저, Xavier 초기화 또는 He 초기화와 같은 방법을 사용하여 파라미터 가중치를 초기화하는 옵티마이저를 포함합니다.
      - 데이터 하이퍼파라미터
        - 데이터의 속성과 관련이 있으며, 데이터가 충분하지 않거나 데이터 변형(자르기, 크기 조정과 같은 데이터 증가 기법)이 충분하지 않을 때 자주 사용됩니다.
- 기계 학습 구현 및 운영
  - 고가용성 및 내결함성
    - 고가용성 솔루션에서는 아키텍처의 구성 요소가 작동을 멈추더라도 시스템이 계속 작동합니다. 고가용성의 중요한 측면 중 하나인 내결함성이 아키텍처에 내장되어 있으면 아키텍처의 구성 요소가 완전히 고장 나더라도 성능 저하 없이 애플리케이션이 계속해 실행되도록 보장합니다.
  - API 호출 및 관련 이벤트를 캡처하는 AWS CloudTrail
    - AWS CloudTrail은 AWS 계정에서 수행하거나 AWS 계정을 대신하여 수행한 API 호출 및 관련 이벤트를 캡처하고 사용자가 지정한 Amazon S3 버킷에 로그 파일을 전송합니다.
    - AWS를 호출한 사용자와 계정, 호출이 이루어진 소스 IP 주소, 호출이 발생한 시간을 파악할 수 있습니다.
  - 주요 AWS 서비스 및 기능을 활용하여 개별 구성 요소의 장애를 설계할 수 있음
    - AWS Glue와 AWS EMR
      - ETL 프로세스를 기계 학습 파이프라인과 분리해야합니다. 기계 학습에 필요한 컴퓨팅 파워는 ETL 프로세스에 필요한 것과 동일하지 않습니다. 요구 사항이 매우 다릅니다.
      - ETL 프로세스에서는 여러 형식의 파일을 읽고 필요에 따라 변환한 다음 영구 스토리지에 다시 기록해야 합니다. 읽기와 쓰기는 많은 메모리와 디스크 I/O를 필요로 하므로 ETL 프로세스를 분리할 때 대량의 ETL 데이터를 쉽게 처리할 수 있는 Apache Spark와 같은 프레임워크를 사용합니다.
      - 반면, 훈련에는 훈련 요구 사항을 처리하는 데 CPU보다 훨씬 더 적합한 GPU가 필요할 수 있습니다. 그러나 GPU는 모형이 훈련되지 않을 때 계속 실행하기에는 경제성이 떨어집니다. 따라서 AWS Glue 또는 Amazon EMR과 같은 ETL 서비스를 사용하기만 하면 이 분리된 아키텍처를 사용할 수 있습니다. ETL 서비스는 ETL 작업에 Apache Spark를 사용하고 Amazon SageMaker를 사용하여 모형을 훈련, 테스트 및 배포합니다.
    - Amazon SageMaker 엔드포인트
      - 고가용성 기계 학습 서비스 엔드포인트를 보장하려면 가용 영역 전체에 걸쳐 여러 인스턴스로 지원되는 Amazon SageMaker 엔드포인트를 배포하십시오.
    - Amazon SageMaker
      - Amazon SageMaker를 사용하면 훈련 및 추론 모두를 위한 기계 학습 모형을 손쉽게 컨테이너식으로 만들 수 있습니다. 그 과정에서 소결합된 분산 서비스로 구성된 기계 학습 모형을 생성하고 원하는 수의 플랫폼에 배치하거나 애플리케이션이 분석하고 있는 데이터 가까이에 배치할 수 있습니다.
    - AWS Auto Scaling
      - AWS Auto Scaling을 사용하면 애플리케이션으로 전송되는 트래픽의 변화에 대응하여 애플리케이션의 일부인 Amazon SageMaker 엔드포인트 등의 AWS 리소스에 대한 Auto Scaling을 구성함으로써 확장 가능한 솔루션을 구축할 수 있습니다.
      - AWS Auto Scaling을 사용하면 확장 조정 계획을 통해 리소스에 대한 확장 조정을 구성하고 관리할 수 있습니다. 조정 계획은 동적 조정과 예측 조정을 사용하여 애플리케이션의 리소스를 자동으로 조정합니다.
  - Amazon SageMaker에 통합된 보안 기능
    - 인증 - IAM 연동
    - 통찰력 확보 - IAM 정책 및 조건 키로 액세스 제한
    - 감사
      - AWS CloudTrail에 대한 API 로그
      - InvokeEndpoint 예외
    - 저장된 데이터 보호
      - AWS KMS 기반 암호화
        - 노트북
        - 훈련 작업
        - modelsEndpoint를 저장할 Amazon S3 위치
    - 데이터 보호 작동 방식
      - HTTPS
        - API/콘솔
        - 노트북
        - VPC 지원
        - 인터페이스 인터페이스
        - IPTraining 작업/엔드포인트별 제한
    - 규정 준수 프로그램
      - BAA
      - ISO 적격 PCI
      - DSS HIPAA
  - 기계 학습 솔루션 배포 및 운영
    - 엔드 투 엔드 테스트 및 A/B 테스트
    - API 버전 관리(여러 버전의 모형이 사용되는 경우)
    - 안정성 및 장애 조치
    - 지속적인 유지 관리
    - CI/CD(지속적 통합/지속적 배포)와 같은 클라우드 인프라 모범 사례
  - Amazon SageMaker 호스팅 서비스를 사용하여 모형을 배포하는 프로세스는 3단계 프로세스입니다.
    - Amazon SageMaker에서 모형 생성
      - 필요한 사항
        - 모형 아티팩트가 저장되는 Amazon S3 경로
        - 추론 코드가 포함된 이미지의 Docker 레지스트리 경로
        - 후속 배포 단계에 사용할 수 있는 이름
    - HTTPS 엔드포인트의 엔드포인트 구성 생성
      - 필요한 사항
        - 프로덕션 변형의 하나 이상의 모형 이름
        - Amazon SageMaker가 각 프로덕션 변형을 호스트하기 위해 시작하도록 하려는 기계 학습 컴퓨팅 인스턴스 프로덕션에서 모형을 호스팅할 때 엔드포인트를 구성하여 배포된 기계 학습 컴퓨팅 인스턴스를 탄력적으로 크기 조정할 수 있습니다.
    - HTTPS 엔드포인트 생성
      - 엔드포인트 구성을 Amazon SageMaker에 제공해야 합니다. 그러면 이 서비스가 기계 학습 컴퓨팅 인스턴스를 시작하고 구성에 지정된 대로 하나 이상의 모형을 배포합니다.
  - 기계 학습 모형을 프로덕션 환경에 제공할 때 주의 사항
    - 소프트웨어 엔지니어링 원칙을 적용합니다. 오류 복구 코드를 추가하고 예기치 않은 데이터 입력에 대한 테스트가 있는지 확인합니다. 다른 시스템에 대해 수행되는 단위 테스트, 품질 보증 및 사용자 수락 테스트와 동일한 종류의 테스트를 수행합니다. 기계 학습 시스템이 연구 단계에서 개발 단계로 진행된 경우, 이러한 권장 소프트웨어 엔지니어링 방식 중 일부가 일관되게 적용되었을 수 있습니다. AWS CodeBuild 및 AWS CodeCommit과 같은 일반적인 DevOps 도구를 사용하여 이 시스템을 자동화합니다.
    - 데이터 원본의 변경 사항을 추적, 식별 및 처리합니다. 데이터는 시간이 지남에 따라 변경될 수 있습니다. 단일 소스에서 데이터 유형을 변경하면 전체 파이프라인이 손상될 수 있습니다. 데이터 원본을 생성하는 소프트웨어가 변경될 경우 연쇄적인 영향을 미칠 수 있습니다.
    - 결과에 대한 지속적인 모니터링 및 평가를 수행합니다. 기계 학습 시스템의 결과를 기대치와 비교해 평가합니다. 프로젝트의 기대치에 대한 오류율 및 오류의 클래스를 확인하는 방법을 구축합니다. 전체 오류율이 동일한 경우, 여러 오류 등급의 비율이 동일합니까? 모형 드리프트가 발생합니까?
    - 향후 모형을 개선하는 데 사용할 수 있는 프로덕션 추론에서 데이터를 수집하는 방법을 만듭니다.
  - Amazon CloudWatch 지표를 사용하는 척도화 정책 정의 및 적용
    - Auto Scaling은 정책을 사용하여 실제 워크로드에 따라 인스턴스 수를 늘리거나 줄임
    - AWS Management Console을 사용하여 미리 정의된 지표에 따라 척도화 정책을 적용할 수 있음
    - 미리 정의돈 지표는 열거형에 정의되어 있으므로 코드에서 이름으로 지정하거나 콘솔에서 사용할 수 있음
    - 항상 Auto Scaling 구성을 로드 테스트하여 프로덕션 트래픽을 관리하는 데 사용하기 전에 올바르게 작동하는지 확인
  - 서비스 FAQ
    - 다음은 기계 학습 관련 AWS 서비스에 대한 이해도를 높이는 데 유용한 정보를 제공하는 서비스 FAQ의 링크입니다.
    - [AWS SageMaker FAQ](https://aws.amazon.com/ko/sagemaker/faqs/)
    - [AWS Comprehend FAQ](https://aws.amazon.com/ko/comprehend/faqs/)
    - [AWS Lex FAQ](https://aws.amazon.com/ko/lex/faqs/)
    - [AWS Polly FAQ](https://aws.amazon.com/ko/polly/faqs/)
    - [AWS Rekognition FAQ](https://aws.amazon.com/ko/rekognition/faqs/)
    - [AWS Translate FAQ](https://aws.amazon.com/ko/translate/faqs/)
    - [AWS Transcribe FAQ](https://aws.amazon.com/ko/transcribe/faqs/)
    - [AWS DeepLens FAQ](https://aws.amazon.com/ko/deeplens/faqs/)
  - AWS 블로그
    - [AWS Machine Learning 블로그](https://aws.amazon.com/ko/blogs/machine-learning/)

# AWS Sagemaker FAQ
- 지도 학습
  - SageMaker Amazon은 분류 또는 회귀 문제에 사용할 수 있는 몇 가지 내장된 범용 알고리즘을 제공합니다.
  - AutoGluon-표 형식 - 모델을 앙상블하고 여러 레이어로 쌓아 승계하는 오픈 소스 AutoML 프레임워크.
  - CatBoost - 그라디언트 부스트 트리 알고리즘의 구현으로, 순서가 있는 부스팅과 범주형 특성 처리를 위한 혁신적인 알고리즘의 구현.
  - Factorization Machine 알고리즘 - 고차원 저밀도 데이터 세트 내 특성 간 상호 작용을 경제적으로 캡처하도록 설계된 선형 모델의 확장.
  - K-Nearest Neighbors(k-NN) 알고리즘 - 레이블이 지정된 가장 가까운 k개의 포인트를 사용하여 분류할 새 데이터 포인트에 레이블을 할당하거나 회귀를 위해 가장 가까운 k개의 포인트 평균을 바탕으로 예측된 대상 값을 지정하는 비모수적 방법.
  - LightGBM - 효율성과 확장성을 개선하기 위한 두 가지 새로운 기법인 그라디언트 기반 단측 샘플링(GOSS)과 배타적 특성 번들링(EFB)이 추가된 그라디언트 부스트 트리 알고리즘의 구현.
  - 선형 학습자 알고리즘 - 회귀를 위한 선형 함수 또는 분류를 위한 선형 임계값 함수를 학습합니다.
  - TabTransformer—Transformer를 기반으로 구축된 새로운 딥 테이블 형식 데이터 모델링 아키텍처. self-attention-based
  - XGBoost 알고리즘 - 더 간단하고 약한 모델 세트의 예상치 앙상블을 결합하는 그라디언트 부스트 트리 알고리즘의 구현.
  - SageMaker 또한 Amazon은 기능 엔지니어링 및 시계열 데이터로부터 예측을 수행하는 동안 보다 전문화된 작업에 사용되는 몇 가지 내장된 지도 학습 알고리즘을 제공합니다.
  - Object2Vec 알고리즘 - 특성 추출에 사용되는 고도로 사용자 지정이 가능한 새로운 다목적 알고리즘. 고차원 객체의 저차원 밀도 임베딩을 학습하여 다운스트림 모델의 훈련 효율성을 향상시키는 특성을 생성할 수 있습니다. 이 알고리즘은 훈련에 레이블이 지정된 데이터가 필요하기 때문에 지도 알고리즘이지만, 사람의 명시적인 주석 없이 데이터의 자연스러운 클러스터링을 통해서만 관계 레이블을 얻을 수 있는 시나리오가 많이 있습니다.
  - DeepAR Forecasting 알고리즘 - 반복 신경망(RNN)을 사용하여 스칼라(1차원) 시계열을 예상하는 지도 학습 알고리즘.
- 비지도 학습
  - SageMaker Amazon은 클러스터링, 차원 축소, 패턴 인식 및 이상 탐지와 같은 다양한 비지도 학습 작업에 사용할 수 있는 여러 내장 알고리즘을 제공합니다.
  - Principal Component Analysis(PCA) 알고리즘 - 처음 몇 개의 주요 구성 요소에 데이터 포인트를 프로젝션하여 데이터 세트 내의 차원(특성 수)을 줄입니다. 목표는 가능한 한 많은 정보나 변형을 유지하는 것입니다. 수학자의 경우 주요 구성 요소은 데이터 공분산 행렬의 고유벡터입니다.
  - k-means 알고리즘 - 데이터 내 별도의 그룹화를 찾습니다. 이 경우 그룹의 멤버는 가급적 다른 멤버와 유사하고 다른 그룹의 멤버와는 가급적 다릅니다.
  - IP Insights - IPv4 주소의 사용 패턴을 학습합니다. IPv4 주소와 여러 엔터티(예: 사용자 ID 또는 계정 번호) 간 연결을 캡처하도록 설계되어 있습니다.
  - Random Cut Forest(RCF) 알고리즘 - 데이터 세트 내에서 제대로 구조화되거나 패턴이 있는 데이터와 다른 비정상적인 데이터 포인트를 탐지합니다.
- 텍스트 분석
  - SageMaker 자연어 처리, 문서 분류 또는 요약, 주제 모델링 또는 분류, 언어 전사 또는 번역에 사용되는 텍스트 문서의 분석에 맞게 조정된 알고리즘을 제공합니다.
  - BlazingText 알고리즘 - 대규모 데이터 세트로 쉽게 확장할 수 있는 Word2vec 및 텍스트 분류 알고리즘의 고도로 최적화된 구현. 많은 다운스트림 자연어 처리 (NLP) 태스크에 유용합니다.
  - Latent Dirichlet Allocation(LDA) 알고리즘 - 여러 문서에서 주제를 결정하는 데 적합한 알고리즘. 비지도 알고리즘으로 훈련 중 답이 포함된 예제 데이터를 사용하지 않습니다.
  - Neural Topic Model(NTM) 알고리즘 - 신경망 접근 방식을 사용하여 여러 문서에서 주제를 결정하는 또 다른 비지도 기법.
  - 텍스트 분류 - TensorFlow - 텍스트 분류에 사용할 수 있는 사전 훈련된 모델을 사용하여 학습 전송을 지원하는 지도 알고리즘.
- 이미지 처리
  - SageMaker 또한 이미지 분류, 객체 감지 및 컴퓨터 비전에 사용되는 이미지 처리 알고리즘도 제공합니다.
  - 이미지 분류 - MXNet - 답이 포함된 예제 데이터를 사용합니다(지도 알고리즘이라고 함). 이 알고리즘을 사용하여 이미지를 분류할 수 있습니다.
  - 이미지 분류 - TensorFlow—사전 훈련된 TensorFlow Hub 모델을 사용하여 특정 작업에 맞게 미세 조정합니다 (감독형 알고리즘이라고 함). 이 알고리즘을 사용하여 이미지를 분류할 수 있습니다.
  - 의미 체계 분할 알고리즘 - 컴퓨터 비전 응용 분야를 개발하는 데 세분화된 픽셀 수준 접근 방식을 제공합니다.
  - 객체 감지 - MXNet - 단일 심층 신경망을 사용하여 이미지의 물체를 감지하고 분류합니다. 이 알고리즘은 입력으로 이미지를 가져와 이미지 장면 내에서 객체의 모든 인스턴스를 식별하는 지도 학습 알고리즘입니다.
  - 물체 감지 - TensorFlow - 이미지에서 경계 상자와 객체 레이블을 감지합니다. 사용 가능한 사전 훈련된 모델을 사용하여 전이 학습을 지원하는 지도 학습 알고리즘입니다.

# Low-Code Machine Learning on AWS
- ML 기본 사항
  - 지도 학습
    - 회귀
      - 회귀에서 문제는 연속 값을 예측하는 것입니다. 수치(독립 변수)와 출력(종속 변수) 간 관계를 이해하기 위한 여러 다양한 방법을 사용할 수 있습니다.
      - 선형 회귀는 지정된 독립 변수 값에 대해 종속 변수 값을 예측하는 데 사용됩니다. 데이터 포인트 그래프에 맞춰진 직선을 사용합니다.
      - 의사결정 트리는 여러 관찰 결과에 대한 결론을 도출하기 위한 예측 모델로 사용됩니다.
      - 랜덤 포레스트는 대규모 의사결정 트리 모음입니다. 회귀 태스크의 경우 개별 트리의 평균 예측이 반환됩니다.
      - 신경망회귀는 인공 신경망이 연속 숫자 값을 모델링하고 예측하는 데 사용되는 기법입니다.
    - 분류
      - 분류에서 문제는 항목이 특정 범주 또는 클래스에 속한다고 예측하는 것입니다. 특정 입력 데이터 예제에 대해 클래스 레이블이 예측되는 다른 예측 모델링을 사용할 수 있습니다.
      - 로지스틱 회귀는 지정된 독립 변수 데이터 집합을 기준으로 이벤트가 발생할 가능성을 예측하는 데 사용됩니다. 일반적으로 예측은 1 또는 0, yes 또는 no, true 또는 false, 긍정 또는 부정과 같은 이진 출력을 나타냅니다.
      - 서포트 벡터 머신은 분류 및 회귀 분석을 위해 데이터를 분석하는 학습 알고리즘이 연결된 지도 학습 모델입니다.
      - 나이브 베이즈는 이벤트의 가능성을 설명하는 알고리즘으로, Thomas Bayes의 이름을 딴 베이즈의 정리를 기준으로 하는 알고리즘입니다. 지정된 해당 클래스 변수 값에 대해 모든 수치 쌍 간의 조건부 독립성에 대한 나이브 가정을 수행합니다.
  - 비지도 학습
    - 클러스터링
      - 클러스터링에서 해결 과제는 개체를 비슷한 속성이 있는 멤버의 그룹(또는 클러스터)으로 구성한 다음, 클러스터에 특성을 지정하는 것입니다.
      - K-평균 클러스터링은 중심 기반 클러스터링 알고리즘입니다. 이 알고리즘에서는 각 데이터 포인트와 중심 간 거리가 계산되어 데이터 포인트를 클러스터에 할당합니다.
      - 주제 모델링 방법은 생성된 주제 클러스터를 포함한다고 간주되는 일반적인 주제를 검색하는 데 사용됩니다.
    - 차원 축소
      - 특성이 많은 훈련 모델은 비용이 크게 들며 과적합이 발생하기가 더 쉽습니다. 차원 축소는 대표적인 특성 또는 영향을 미치는 특성의 수를 최소화합니다.
      - 특성 선택을 사용하여 영향을 거의 미치지 않는 특성을 식별합니다. (이러한 특성을 고려하지 않고 제외할 수 있습니다.)
      - 특성 추출을 사용하여 동급의 표현 능력을 갖는 특성(원시 특성 조합)을 파생합니다.
  - 강화 학습
    - 강화 학습에서는 기계에 성과 점수(지침으로서)와 훈련 데이터의 일부에만 레이블이 지정되는 반지도 학습이 제공됩니다.
- ML 비즈니스 검토에서 고려해야 하는 몇 가지 단계
  - 비즈니스 요구 사항을 이해합니다.
  - 비즈니스 질문을 작성합니다.
  - 프로젝트의 ML 타당성과 데이터 요구 사항을 검토합니다.
  - 데이터 획득, 교육, 추론 및 잘못된 예측에 따른 비용을 평가합니다.
  - 사용 가능한 경우 비슷한 영역의 입증되었거나 발표된 결과물을 검토합니다.
  - 허용 가능한 오류를 포함하여 핵심 성능 지표를 확인합니다.
  - 비즈니스 질문에 따라 기계 학습 태스크를 정의합니다.
  - 중요한 필수 특성을 식별합니다.
  - 소규모의 중심 개념 증명(POC)을 설계하여 이 목록의 모든 이전 항목을 검증합니다.
  - 외부 데이터 소스를 가져오면 모델 성능이 향상되는지 평가합니다.
  - 프로덕션에 대한 경로를 설정합니다.
  - 구현으로 발생할 수 있는 새로운 비즈니스 프로세스를 고려합니다.
  - 관련 이해관계자에게 이니셔티브에 관한 최신 정보를 제공합니다.
- ML 수명 주기
  - ML 프로세스
    - 비즈니스 목표 식별
    - ML 문제 구성
    - 데이터 수집
    - 데이터 통합 및 준비
    - 특성 추출
    - 모델 훈련
    - 모델 검증
    - 비즈니스 평가
    - 프로덕션 배포(모델 배포 및 모델 추론)
- 수명 주기 단계
  - 프로젝트 범위: 비즈니스 문제 및 목표 정의
    - 영역 전문가 및 비즈니스 소유자는 이 단계와 관련이 있으며 성공 지표를 결정합니다.
    - 핵심 성과 지표(KPI)를 식별하고 규정 준수 및 규제 요구 사항을 결정하는 일도 이 단계에 포함됩니다.
  - 데이터 준비: 모든 관련 데이터 수집 및 준비
    - 다양한 데이터 소스의 모든 관련 데이터를 수집하고 준비하는 과정이 포함됩니다. 데이터의 추출, 변환 및 로드(ETL)를 위한 빅 데이터 기법을 사용해 본 경험이 있는 데이터 엔지니어가 주로 이 역할을 맡습니다.
    - 감사 및 규정 준수 요구를 충족하려면 데이터의 버전이 관리되고 데이터의 계보가 기록되는지 확인하는 것이 중요합니다.
    - Data Scientist는 데이터를 탐색하고, 입력 특성 및 목표 변수를 식별하고, 이상값 분석을 수행하고, 원시 데이터 집합에 대한 액세스 권한을 얻은 후에 필요한 데이터 변환을 수행합니다. 또한 Data Scientist가 훈련 데이터에 대해 수행한 변경 사항을 프로덕션에서 추론에 사용할 수 있도록 하는 것도 중요합니다.
  - 모델 개발: 모델 개발 및 평가
    - Data Scientist는 사용할 프레임워크를 결정합니다. 또한 다음과 같이 샘플 외 데이터 집합 및 시간 외 데이터 집합을 다음과 같이 정의합니다.
      - 샘플 외 데이터는 개발 및 최적화 프로세스 동안 사용되지 않은 데이터입니다. 이 데이터는 모델 성능을 평가하는 데 사용됩니다.
      - 시간 외 데이터는 모델 개발에 사용한 기간과는 완전히 다른 기간 동안 획득한 데이터입니다.
    - Data Scientist는 다음 작업을 통해 실험합니다.
      - 다른 ML 알고리즘 사용
      - 학습 프로세스를 제어하는 데 사용되는 값이 있는 파라미터인 하이퍼파라미터 사용
      - 별도의 훈련 데이터 추가(일부 경우에 해당)
  - 모델 배포: 데이터 집합에 대해 학습된 모델 실행
    - 이 단계에서는 학습된 모델을 사용하고, 시간 외 및 샘플 외 데이터 집합에 대해 실행합니다.
    - 그런 후 1단계에서 결정한 지표와 가장 비슷한 최상의 결과를 반환하는 모델을 선택합니다.
    - 모델 아티팩트 및 해당 코드의 버전을 적절히 관리하고 중앙 집중식 코드 리포지토리 또는 아티팩트 관리 시스템에 저장해야 합니다.
    -  Data Scientist는 모델 성능이 일관되게 불량한 경우 데이터 수집 또는 특성 추출 단계로 돌아갈 수 있습니다. 또한 Data Scientist는 나쁜 성능의 이유를 제공하거나 특정 또는 모델이 예측에 미치는 영향을 설명해야 합니다.
  - 모델 모니터링 및 유지 관리: 프로덕션으로 모델 배포
    - DevOps 엔지니어, Data Scientist, 데이터 엔지니어, 영역 전문가, 최종 사용자 및 비즈니스 소유자가 포함됩니다. 표준화된 지표가 있어야 하며, 모든 의사결정 수립자는 이러한 지표를 직접적으로 해석할 수 있어야 합니다.
  - MLOps의 이점
    - MLOps 모델은 데이터 과학, 프로덕션 및 운영 팀이 가능한 한 자동화되는 ML 워크플로에서 함께 원활하게 작업할 수 있도록 합니다. 필요에 따라 인적 개입을 통합하여 원활한 배포, 효과적이며 지속적인 데이터 모니터링 및 모델 성능 추적을 보장할 수 있습니다.
    - ML 프로젝트의 시장 출시 간격(TTM)이 훨씬 단축됩니다.
    - 혜택
      - 반복성
        - 기계 학습 개발 수명 주기(MLDC)의 모든 단계를 자동화하면 반복 가능한 프로세스를 확실하게 수행할 수 있음.
        - 반복 가능한 프로세스는 모델을 훈련시키고 평가하고 버전을 관리하고 배포하는 방식을 포함함
      - 감사 가능성
        - 데이터 과학 실험부터 소스 데이터, 훈련된 모델에 이르기까지 모든 입력과 출력의 버전을 관리할 수 있음.
        - 모델이 빌드된 방식과 배포된 위치를 정확히 시연할 수 있음
      - 생산성
        - 큐레이팅한 데이터 집합에 액세스할 수 있는 셀프 서비스 환경을 제공하여 데이터 엔지니어 및 Data Scientist가 더 빠르게 이동하며, 누락된 데이터 또는 잘못된 데이터로 인한 시간 낭비를 줄이는 데 도움을 줌
      - 신뢰성
        - CI/CD 사례를 통합하여 높은 품질과 일관성으로 빠르게 배포할 수 있음
      - 데이터 및 모델 품질
        - MLOps를 사용하여 모델 편향을 방지하는 정책을 적용할 수 있음. 시간에 따른 데이터 통계적 속성 및 모델 품질 변화를 추적함
- 데이터 준비 과제
  - 4개의 범주를 각각 확장하여 데이터 관련성 과제
  - 데이터 연관성
    - 편향된 데이터
      - 데이터의 편향은 특정 데이터 집합 요소가 과도하게 가중치가 부여되거나 과도하게 표현될 때 발생하는 오류입니다.
      - 편향된 데이터 집합은 ML 모델의 사용 사례를 정확히 나타내지 않으며 이로 인해 정확하지 않은 결과, 조직적인 편견 및 낮은 정확도가 발생합니다.
      - 성별에 따른 고객의 선호도를 포함하는 데이터 집합을 고려하십시오. 동일하지 않은 항목 수를 유지하면 대부분의 ML 모델이 편향됩니다.
    - 데이터에 대한 계절적 효과
      - 명확한 계절적 효과가 있는 많은 데이터 예제가 있습니다. 예를 들어 고객의 행동이나 재무 데이터는 일반적으로 추세를 보입니다. 이러한 해결 과제를 위해서는 하위 집합을 신중하게 선택하거나, 데이터 집합에서 계절적 효과를 유발하는 값을 제외하는 디트렌드 절차를 구현해야 합니다.
    - 시간 경과에 따른 데이터 변경
      - 일반적으로 시간에 따라 관찰 결과가 달라집니다.
      - 이전 데이터는 최신 데이터와 비슷하지 않은 프로세스를 설명하므로 잘못된 결과로 이끄는 경우가 많습니다. 이러한 데이터 속성에는 데이터 집합 교체가 필요할 수 있습니다.
    - 훈련 및 테스트 하위 집합의 관련성
      - 편향된 훈련 데이터와 마찬가지로, 가능한 모든 이벤트와 특성 중 일부만을 고려하는 테스트 데이터를 사용하면 부정확한 결과가 산출될 수 있습니다.
      - 통계적으로 올바르지 않은 편향된 데이터 집합을 훈련시킨 ML 모델로 인해 오류가 있는 모델이 생성될 수 있습니다. 따라서 편향되지 않은 완전한 테스트 및 훈련 데이터를 유지하는 것이 중요합니다.
      - 이러한 문제를 피할 수 있는 한 가지 방법은 다양한 테스트 집합을 사용하는 것입니다. 또 다른 방법은 기본 데이터 집합에서 하나의 데이터를 더 작은 규모로 동일한 정보 및 특성을 포함하는 하위 샘플로 선택하는 것입니다.
  - 데이터 정리
    - 이상값
    - 중복 항목
    - 누락된 값
      - 누락된 데이터가 있는 레코드를 제거하거나, 누락된 값을 교체하거나, 특수한 알고리즘(예, 평균값, 모드 및 0으로 교체)을 적용할 수 있습니다.
    - 데이터의 노이즈
      - 단어 노이즈는 일반적으로 숫자 값을 설명하는 데 사용되지만, 텍스트 또는 다른 데이터를 설명할 수도 있습니다
  - 데이터 규모
    - 데이터양
      - 대량의 데이터 집합을 유지하려면 일반적으로 특수한 데이터 스토리지 기법 또는 이러한 데이터를 처리하기 위한 클라우드 컴퓨팅이 필요합니다.
      - 방대한 하위 집합은 복잡한 개발 인프라를 만들고 훈련 시간을 늘릴 수 있습니다
    - 무작위화된 데이터 하위 집합
      - 적절한 데이터 분할 기법을 적용하는 것은 모델을 훈련시키고 테스트할 때 특히 중요합니다. 주로 엔지니어는 순차적인 데이터 부분을 사용합니다. 이러한 부분은 특정 기간이나 그룹만 설명합니다. 이러한 데이터를 사용하면 ML 모델의 성능이 저하됩니다. 테스트를 무작위화하거나 하위 집합을 훈련시킬 때 Data Scientist가 데이터 분할 기법을 사용하는지 확인하여 이 문제를 피할 수 있습니다.
    - 데이터 표현
    - 데이터 일치
  - 데이터 준비 절차
    - 프로세스에 새 데이터 포함
      - ML 기반 프로젝트를 비롯한 많은 프로젝트는 제시간에 해결되지 않습니다. 모델이 지속적으로 훈련되거나 테스트되는 경우가 많습니다.
      - 시간에 따라 데이터 형식이 달라짐에 따라 새로운 준비 기법을 개발해야 합니다. 때때로 연속 사용을 위해 처리 파이프라인을 조정하려면 상당한 작업이 필요할 수 있습니다.
      - 모델을 새 데이터에 적합한 상태로 유지해야 합니다. 성능(및 시간에 따른 성능 변화)을 지속적으로 확인하고 지속적인 훈련 파이프라인을 빌드합니다.
    - ETL 및 ELT 프로세스 자동화
      - 추출, 변환 및 로드(ETL) 프로세스는 비즈니스 규칙 집합을 사용하여 여러 소스의 데이터를 중앙의 데이터 저장소로 전송하기 전에 데이터를 처리합니다.
      - 추출, 로드 및 변환(ELT) 접근 방식은 데이터를 현재 형식으로 로드하고, 사용 사례 및 분석 요구 사항에 따라 후속 단계에서 변환합니다.
      - 때때로 모델이 작동하는 데 필요한 데이터를 먼저 수집한 후 표준화해야 합니다. 예를 들어, 새 레코드가 데이터 집합에 계속 추가되는 웹 크롤러가 있을 수 있습니다. 과제는 다음과 같습니다.
        - 첫째, 추출 단계가 복잡해집니다.
        - 둘째, 연속 데이터의 변환은 상당히 어려울 수 있습니다.
        - 셋째, 특정 값의 분포를 올바르게 평가할 수 없습니다.
      - 모델 적합을 위해 입력을 제공하는 것은 복잡한 태스크일 수 있습니다. 연속되거나 반복되는 데이터 사용 요소가 있는 경우 데이터 준비 기법이 전체 프로젝트의 통합된 일부가 됩니다. 특수한 배포, 빠른 계산, 최적화된 연산 등이 필요합니다.
- 모델 빌드 선택
  - 알고리즘 선택
    - 데이터 레이블 지정은 원시 데이터(이미지, 텍스트 파일, 비디오 등)를 식별하고 하나 이상의 의미 있는 정보용 레이블을 추가하여 ML 모델이 학습할 수 있도록 컨텍스트를 제공하는 프로세스입니다.
    - 레이블이 지정된 데이터
      - 기본적으로 예측할 결과가 바이너리(0/1)인 경우 로지스틱 회귀 알고리즘을 사용합니다.
      - 기본적으로 예측할 결과가 멀티클래스(1/2/3/...)인 경우 랜덤 포레스트 또는 k-최근접 이웃(KNN)과 같은 의사결정 트리 분류자를 선택합니다.
      - 기본적으로 수량, 돈, 높이 및 무게와 같은 예측할 결과가 연속적이면 선형 회귀 알고리즘을 선택합니다. 이러한 알고리즘의 예제로는 의사결정 트리 회귀자 및 랜덤 포레스트 회귀자가 있습니다.
    - 레이블이 지정되지 않은 데이터
      - 레이블이 지정되지 않았거나 구조화되지 않은(텍스트) 데이터를 분석하기 위해서는 클러스터링(비지도) 알고리즘이 선호됩니다(예: k-평균 클러스터링).
      - 구조화되지 않은 다른(이미지 및 음성) 데이터 유형을 분석하기 위해서는 인공 신경망(ANN) 알고리즘이 제안됩니다. 예제로는 이미지 인식을 위한 컨볼루션 신경망(CNN), 음성 인식을 위한 반복 신경망(RNN) 및 NLP가 있습니다. 더 나은 성능을 위해 회귀 모델(ML 모델)보다 딥 러닝 모델(신경망)을 사용하는 것이 좋습니다. 그 이유는 이러한 모델이 활성화 함수(AF)를 추가하여 비선형성을 추가로 도입하기 때문입니다.
- 과적합 방지
  - 조기 중지
    - 조기 중지는 ML 모델이 데이터의 노이즈를 학습하기 전에 훈련 단계를 일시 중지합니다.
  - 정리
    - 모델을 빌드할 때 최종 예측에 영향을 미치는 몇 가지 특성 또는 파라미터를 식별할 수 있습니다. 특성 선택 또는 정리는 훈련 집합에서 가장 중요한 특성을 식별하고, 관련성이 없는 특성을 제거합니다.
  - 규제화
    - 규제화는 과적합을 줄이려는 훈련 및 최적화 기법 모음입니다. 이러한 방법은 중요도에 따라 특성에 등급을 매겨 예측 결과에 영향을 미치지 않는 요소를 제거하려고 합니다.
  - 앙상블
    - 앙상블은 여러 구분된 ML 알고리즘에서 얻은 예측을 조합합니다. 일부 모델은 결과가 주로 부정확하므로 약한 학습자라고 부릅니다.
    - 앙상블 방법은 모든 약한 학습자를 조합해서 보다 정확한 결과를 얻습니다. 여러 모델을 사용하여 샘플 데이터를 분석하고 가장 정확한 결과를 선택합니다.
    - 두 가지 주요 앙상블 방법은 배깅 및 부스팅입니다. 배깅은 여러 다른 ML 모델을 동시에 훈련시키지만 부스팅은 하나씩 차례로 훈련시켜 최종 결과를 얻습니다.
  - 데이터 증강
    - 데이터 증강은 모델이 샘플 데이터를 처리할 때마다 샘플 데이터를 조금씩 변경하는 ML 기법입니다. 이 기법은 입력 데이터를 조금씩 변경하는 방식으로 수행합니다.
    - 데이터 증강을 적절히 사용하면 훈련 집합이 모델에 고유하게 나타나고 모델이 그 특성을 학습하지 못하게 됩니다.
- 불균형 데이터 처리 기법
  - 적절한 평가 지표 고려
    - 고객은 정밀도와 재현율 간 균형을 유지하는 F1 점수를 고려할 수 있습니다. 분류자가 특정 클래스의 데이터를 더 많이 올바르게 식별하는 경우에만 점수가 높아집니다. AUC는 또 다른 대체 평가 지표입니다.
  - 리샘플링
    - 불균형 데이터 집합에서 균형 데이터 집합을 만들려면 언더샘플링 및 오버샘플링을 사용할 수 있습니다. 언더샘플링은 풍부한 클래스의 크기를 줄여 데이터 집합의 균형을 유지합니다. 반대로, 오버샘플링은 데이터 수량이 충분하지 않을 때 사용됩니다. 이 방법은 드문 샘플의 크기를 늘려 데이터 집합의 균형을 유지합니다.
    - 풍부한 샘플을 삭제하지 않고, 반복, 부트스트래핑 또는 합성 소수 오버샘플링 기법(SMOTE) 등을 사용해서 드문 새 샘플을 생성합니다. SMOTE는 원본 데이터 포인트를 기준으로 합성 데이터 포인트를 만들어 데이터 증강을 수행하는 알고리즘입니다. SMOTE를 고급 버전의 오버샘플링 또는 데이터 증강의 특정 알고리즘으로 볼 수 있습니다. SMOTE는 중복 항목을 생성하지 않으므로 이점이 있습니다. 대신 원본 데이터 포인트와는 약간 다른 합성 데이터 포인트를 만듭니다.
  - K-폴드 교차 검증
    - 교차 검증은 사용 가능한 데이터양이 제한되어 있을 때 ML 모델의 성능을 평가하는 데 사용되는 기법입니다. 이 프로세스에서는 데이터가 k 그룹 또는 폴드로 분할됩니다.
    - 모델은 k-1 폴드가 훈련된 후 나머지 폴더에서 검증됩니다. 이 작업은 각 폴드가 검증 집합으로 한 번씩 사용되도록 k번 반복됩니다. 모든 k 폴드 간 성능의 평균을 계산하여 모델의 전체 기술을 평가합니다. 이 방법은 과적합을 방지하며, 모델 성능에 대한 강력한 예측 기능을 제공합니다.
  - 리샘플링된 다양한 데이터 집합 앙상블
    - 여러 다른 클러스터 노드에서 모델을 훈련시키고 실행할 수 있으므로 많은 데이터가 있는 경우 효율적이며 수평적으로 완전히 확장 가능합니다
    - 데이터를 리샘플링할 때 다른 샘플링 비율을 사용하는 것입니다. 드문 클래스와 풍부한 클래스 간 비율을 세부 튜닝하는 이전 접근 방법은 잘 작동할 수 있습니다
  - 분류자 고려 사항
    - BalancedBaggingClassifier는 scikit-learn(sklearn) 분류자와 동일하지만 밸런싱 기능이 추가되어 있습니다. 여기에는 지정된 샘플러에 대한 적합 시점에 훈련 집합을 밸런싱하기 위한 추가 단계가 포함됩니다.
    - BalancedBaggingClassifier 분류자는 2개의 특별 파라미터인 sampling_strategy 및 replacement를 사용합니다
  - 풍부한 클래스 클러스터링
    - 풍부한 클래스 클러스터링이라는 기법을 사용할 수 있습니다. 다양한 훈련 샘플을 포괄하는 랜덤 샘플을 사용하는 대신, 풍부한 클래스를 r 그룹으로 클러스터링합니다.
    - 여기서 r은 r의 사례 수입니다. 각 그룹에 대해 medoid(클러스터의 중심)만 유지됩니다. 이 모델은 드문 클래스와 medoid로만 훈련됩니다.

## 탐색적 데이터 분석 및 데이터 준비
- SageMaker Data Wrangler
  - SageMaker Data Wrangler는 ML 수명 주기 과제를 해결하기 위해 특별히 설계된 데이터 집계 및 준비 도구입니다.
  - 이 도구는 단일 시각적 인터페이스에서 데이터 선택, 정리, 탐색 및 시각화와 같은 데이터 준비 및 특성 추출 프로세스를 간소화합니다
- 데이터 변환
  - JSON 파일 평면화
  - 중복 행 삭제
  - 합성 소수 오버샘플링 기법(SMOTE)과 같은 데이터 균형 기법 사용
  - 평균값 또는 중앙값과 함께 누락된 데이터 입력
  - 원-핫 인코딩 사용
  - 시계열 관련 변환기를 사용하여 ML에 대한 시계열 데이터 준비 가속화
- 데이터 준비 및 특성 추출
  - 자기상관 : 자기상관은 다른 시점의 두 샘플 간 유사성을 확인합니다. 이 방법은 반복되는 주기적 패턴을 식별하는 데 유용합니다.
  - 지연 특성 : 지연 특성은 이전 정보가 향후 정보에 영향을 미칠 수 있다는 가정에 따라 변수를 이전 타임스탬프로 이동할 때 생성됩니다. 이러한 특성은 이전 시간의 변수 행동을 관찰하여 향후의 행동을 이해하는 데 유용합니다.
  - 리샘플링 : 주로 시계열 데이터에는 다른 시간 단계가 포함되며, 세분성이 낮을 때 이러한 현상이 두드러집니다. 리샘플링은 데이터를 규칙적인 시간 단계로 샘플링하여 불규칙적인 시간 단계를 완화하는 데 도움이 됩니다. 이러한 작업은 대부분의 시계열 알고리즘에 유용합니다.
  - 이동 기간 : 모델 파라미터가 시간과 관계없이 일정하다고 가정하는 것이 일반적입니다. 이동 기간 기법은 파라미터에 시간 불변성이 있는지 검증하는 데 도움이 됩니다.

## Amazon SageMaker Autopilot 심층 분석
- Amazon SageMaker Autopilot
  - Amazon SageMaker Autopilot은 기계 학습(ML) 모델 빌드에 필요한 많은수동 작업을 없애줍니다.
  - 테이블 형식 데이터 집합을 제공하고 예측할 대상 열을 선택합니다. SageMaker Autopilot은 여러 솔루션을 자동으로 확인하여 최적 모델을 찾습니다.
  - 데이터 탐색 및 문제 유형에 적합한 알고리즘 선택을 비롯한 ML 프로세스의 핵심 태스크를 자동화합니다.
  - 편리한 모델 훈련 및 튜닝을 위해 데이터 준비 태스크를 자동화합니다
- 다음 방식으로 SageMaker Autopilot을 사용할 수 있습니다.
  - 자동: 데이터를 탐색하고, 문제 유형과 관련성이 있는 알고리즘을 선택하고, 모델 훈련 및 튜닝이 용이하도록 데이터를 준비합니다.
  - 수동: 사용자 지침에 따라 Amazon SageMaker Studio를 사용합니다.
  - 도구 사용: AWS SDK를 사용합니다.
- 지원되는 형식 및 데이터 형식
  - CSV는 데이터를 인간이 판독 가능한 일반 텍스트로 저장하는 행 기반 파일 형식입니다. 이는 광범위한 애플리케이션에서 지원되는 데이터 교환에 자주 사용되는 옵션입니다.
  - Parquet은 데이터가 행 기반 파일 형식보다는 더 효율적으로 저장 및 처리되는 열 기반 파일 형식입니다. 이 형식이 빅 데이터 문제에 더 나은 옵션입니다.
- HPO 모드 알고리즘
  - HPO 모드에서 SageMaker Autopilot은 훈련 알고리즘인 선형 모델, XGBoost 및 딥 러닝 알고리즘 중 하나를 선택합니다.
    - 선형 모델은 분류 또는 회귀 문제를 해결할 수 있는 지도 학습 알고리즘입니다.
    - XGBoost는 대상 변수를 정확하게 예측하려고 시도하는 지도 학습 알고리즘입니다. 이 알고리즘은 덜 복잡하고 더 약한 모델 집합의 예측 앙상블을 조합합니다.
    - 딥 러닝 알고리즘은 다중 계층 인식(MLP) 및 피드포워드 인공 신경망입니다. 이 알고리즘은 선형으로 구분할 수 없는 데이터를 처리할 수 있습니다.

# Building Language Models on AWS (Korean)
## Course Series Introduction
- 대규모 언어 모델 유형
  - 언어 모델
    - 언어 모델은 통계적 메서드를 사용하여 자연 텍스트 시퀀스에서 발생하는 연속 토큰을 예측합니다.
  - 대규모 언어 모델
    - 대규모 언어 모델(LLM)은 수억 개(BERT)에서 1조 개 이상의 파라미터(MiCS)가 포함된 신경망 네트워크 기반 언어 모델입니다. 
    - 이러한 모델은 텍스트에서 패턴과 구조, 의미 체계 관계를 인식할 수 있습니다.
      - 순환 신경망
      - 합성곱 신경망
    - 그러나 가장 널리 사용되는 모드 유형은 변환기 기반 모델입니다. 병렬 처리 기능과 텍스트에서 장거리 종속성을 다루는 데 효율적이므로 LLM 훈련에서 표준으로 허용됩니다.
- 일반적인 생성형 AI 사용 사례
  - 대규모 언어 모델의 생성형 기능은 생성형 인공 지능(생성형 AI)의 핵심입니다. 생성형 AI는 다음과 같이 새 콘텐츠와 아이디어를 만듭니다.
    - 대화
    - 스토리
    - 이미지
    - 동영상
    - 음악

## Addressing the Challenges of Building Language Models
- LLM 실무자의 일반적인 과제
  - 훈련 데이터 엄선
    - BERT, GPT(예: GPT-4), Jurassic-2, T5 등의 변환기 기반 모델은 일반적으로 다양한 데이터 원본에서 자연적으로 발생하는 텍스트의 대규모 데이터 집합에서 훈련됩니다. 
    - LLM의 최종 품질은 어떤 훈련 데이터를 선택하고 엄선하는지에 따라 크게 달라집니다. 
    - LLM 훈련 데이터 준비는 LLM 업계에서 활발한 연구와 혁신이 이뤄지는 분야입니다. 
    - 이러한 텍스트를 수집하고, 처리하고, 정리하려면 리소스가 많이 필요하지만 모델 출력의 품질을 보장하는 데는 필수입니다. 
  - 대규모 최신 인프라 요구 사항
    - LLM을 훈련할 때 데이터 과학자는 모델 크기, 모델 성능, 컴퓨팅 복잡성 등의 여러 요소 간의 균형을 유지해야 합니다. 
    - 훈련은 며칠부터 몇 주까지 지속될 수 있으며 일반적으로 대규모 가속 컴퓨팅 리소스, 고속 네트워킹, 최신 컴퓨팅 인스턴스가 필요합니다. 
    - 이러한 컴퓨팅 인스턴스는 일반적으로 서로 인접해 있으며 단일 네트워크 스파인에 그룹화되는 경우도 있습니다. 
    - GPU 품질 관리 소프트웨어는 실패를 탐지하고 처리하여 데이터 집합과 모델 체크포인트의 분산 스토리지 및 다중 노드 데이터 I/O를 구성하는 데 필수입니다. 
  - 비싼 훈련 비용
    - 조직은 LLM을 훈련하는 데 수백만 또는 수십억 달러의 비용을 투자해야 하며, 이러한 투자를 할 수 없는 조직도 있습니다. 
    - 따라서 팀에서는 비용 효율적인 훈련 솔루션을 찾거나 사전 훈련된 모델을 미세 조정하는 방법을 선호합니다. 
  - 기계 학습 전문 지식
    - LLM 실무자들은 성능 최적화와 일반화를 추구하면서 분산 훈련과 병렬로 데이터를 처리하는 고급 기술을 사용합니다. 
    - 또한 이러한 실무자들은 인프라를 이해하고 관리해야 합니다. 
    - 이를 사용하려면 도구와 기계 학습 개념, 인프라에 대해 심층적인 전문 지식을 갖춰야 합니다.
  - 책임감 있는 방식으로 작동하는 AI
    - 복잡한 모델의 추론을 이해하는 것은 어려운 작업입니다. 
    - 언어 모델이 공정하고 투명하며 편향적이지 않도록 확인하기 위한 활발한 연구가 진행 중입니다. 
    - 다양한 작업에서 모델의 성능을 평가하고 비교하기 위해 적절한 벤치마크를 만드는 것 역시 활발한 연구 분야의 하나입니다.
- 분산 훈련으로 LLM 크기 조정
  - 리소스 사용률을 극대화하고 비용을 줄이기 위해 LLM 실무자들은 대개 다중 GPU 또는 여러 기계 훈련에 분산 컴퓨팅 기술을 사용합니다. 
  - 분산 데이터 병렬 처리와 분산 모델 병렬 처리 기술을 예로 들 수 있습니다. 
  - 이 방법을 사용하면 수평적 크기 조정, 병렬 처리, 내결함성, 효과적인 리소스 사용에 도움이 됩니다.
- 데이터 병렬 처리
  - 데이터 병렬 처리는 일반적으로 데이터가 하나의 디바이스(예: GPU 1개)로는 적합하지 않을 때 사용됩니다. 
  - 이 기술을 사용하면 모델의 복사본이 포함된 여러 디바이스에서 데이터 집합이 샤드됩니다. 
  - 훈련 단계가 시작될 때 미니 패치는 중첩되지 않는 방식으로 모든 모델 복제본에 균등하게 배포됩니다. 
  - 그런 다음 복제복이 병렬로 훈련되고 모델 파라미터는 디바이스 전체에서 지속적으로 동기화됩니다. 
  - 일반적으로 공통적인 통신 알고리즘과 전문화된 고성능 컴퓨팅(HPC) 네트워킹 인프라를 사용하여 파라미터 동기화와 효과적인 디바이스 간 통신이 구현됩니다.
  - 데이터 병렬 처리에 대한 방식은 여러 가지가 있으며, 몇 가지 일반적인 방식을 아래에서 확인할 수 있습니다.
    - AllReduce
      - AllReduce 방식은 디바이스 간의 직접 통신을 기반으로 통신 기본 요소를 사용하여 모델 그래디언트와 파라미터를 상호 교환합니다. 
      - 여기서는 디바이스에서 데이터를 집계하고 집계된 결과를 다시 재배포합니다. 
      - 노드는 이러한 동기화를 통해 일관된 방식으로 모델 파라미터를 일괄적으로 업데이트합니다. 
      - 이 방식은 Horovod에서 이 알고리즘의 일명 ring all-reduce 변형이 GPU 128개에서 대규모 CNN 모델을 훈련할 때 거의 90%에 가까운 크기 조정 효율성을 입증한 이후로 널리 사용되게 되었습니다.
    - 파라미터-서버 방식
      - 이 방식에서는 로컬 모델 복제본이 전용 파라미터 서버 집합 간의 푸시앤드풀 의미 체계를 사용하여 동기화됩니다. 
      - 서버는 명시적으로 모델 파라미터의 가장 최신 전체 복사본을 보관하거나 가중치 평균화 단계에 참여합니다. 
      - 예를 들어 이 프로세스는 각 훈련 단계가 끝날 때 모든 모델 복제본에서 동기식으로 수행되거나, 복제본이 서로 별개로 파라미터를 가져오고 그래디언트를 푸시하는 비동기식으로 수행될 수 있습니다.
      - Elastic Fabric Adapter(EFA)와 같은 HPC 인프라 구성 요소를 도입하여 알고리즘 최적화를 통해 파라미터-서버 방식의 성능이 향상되었습니다. 
      - Amazon의 파라미터-서버 기반 Herring 라이브러리는 all-reduce 기반 방식보다 훨씬 뛰어난 성능을 보였으며, GPU 2,048개 전반의 대규모 BERT 모델 훈련에서 전례 없는 85%의 크기 조정 효율성을 달성했습니다. 
- 모델 병렬 처리
  - 모델 병렬 처리는 신경망이 너무 커서 단일 디바이스(예: GPU 1개)에 맞지 않거나, 훈련 프로세스에서 메모리를 덜 사용하도록 할 때 알맞습니다. 
  - 모델 병렬 처리에서 딥 러닝 모델은 여러 디바이스, 여러 인스턴스 내 혹은 인스턴스 간에 파티셔닝되어 훈련 클러스터의 통합 GPU 메모리를 효율적으로 활용하고 전체 모델을 메모리 효과적인 방식으로 저장합니다.
  - 파이프라인 병렬 처리
    - 파이프라인 병렬 처리는 여러 디바이스 전반의 모델 계층이나 작업 집합을 파티셔닝하고 훈련 미니 배치를 마이크로 배치로 분할합니다. 
    - 이는 디바이스 유휴 시간을 최소화할 수 있는 중첩 방식으로 마이크로 배치가 정방향 및 역방향으로 컴퓨팅되도록 예약되는 인공 파이프라인을 생성합니다.
  - 텐서 병렬 처리
    - 이 모델 유형에서는 병렬 처리 모델 가중치, 그래디언트, 옵티마이저 상태가 여러 디바이스에 분할됩니다.
    - 각 가중치 전부를 유지하되 가중치 집합을 파티셔닝하는 파이프라인 병렬 처리와 달리 텐서 병렬 처리는 각 가중치를 분할합니다. 
    - 일반적으로 일부 작업의 분산 컴퓨팅, 모듈, 모델 계층이 여기에 포함됩니다. 
    - 텐서 병렬 처리는 단일 파라미터에서 대부분의 GPU 메모리가 사용되거나 수십 개의 인스턴스에 걸쳐 파티셔닝되어야 하는 GPT-3 등의 초대형 모델인 경우에 유용합니다.
  - AWS의 Amazon SageMaker에서는 데이터 병렬과 모델 병렬 라이브러리를 제공하여 사용자가 최소한의 코드 변경으로 데이터 병렬 처리와 모델 병렬 처리를 구현하도록 지원합니다. 모델 병렬 처리 기술을 사용할 수 있는 다른 현대적 분산 훈련 프레임워크로는 Microsoft의 DeepSpeed와 NVIDIA의 Megatron-LM을 예로 들 수 있습니다. NVIDIA의 Megatron-LM은 SageMaker에서도 사용할 수 있습니다.
- 성능 최적화 기술
  - 혼합 정밀도 훈련
    - 이 기술은 낮은 정밀도 계산을 사용하여 리소스 사용률을 줄임으로써 CPU 워크로드를 감소시키고 스토리지 사용량을 최소화합니다. 
    - 이 덕분에 동일한 양의 메모리로 더 큰 네트워크를 배포하거나, 단일 또는 이중 정밀도 네트워크 대비 메모리 사용량을 줄여 성능을 높일 수 있습니다.
  - 그래디언트 체크포인팅
    - 이 기술은 중간 활성화의 하위 집합만 저장하고 역방향 전달 중에 이를 다시 컴퓨팅하여 메모리 소비를 줄입니다.
  - 연산자 결합
    - 이 기술을 사용하면 LLM 실무자는 여러 연산을 단일 연산에 결합하여 메모리 할당과 중간 결과 수를 줄일 수 있습니다. 
- 특별히 설계된 인프라 사용
  - AWS Trainium 및 AWS Inferentia Amazon Elastic Compute Cloud(EC2) 인스턴스
    - Trainium은 AWS에서 딥 러닝 훈련을 고려해 특별히 설계한 2세대 기계 학습 액셀러레이터입니다. 이는 EC2 Trn1 인스턴스를 작동시킵니다. 
    - Inferentia 액셀러레이터는 딥 러닝(DL) 추론 애플리케이션에 최저 비용으로 고성능을 제공합니다. Inferentia2로 작동하는 Inf2 인스턴스는 수천억 개의 파라미터가 포함된 모델이 있는 대규모 생성형 AI 애플리케이션에 특별히 최적화되어 있습니다.
  - 주요 이점
    - LLM 및 확산 모델에서 가장 비용 효율적이고 고성능의 훈련
    - 딥 러닝(DL) 모델 실행 시 클라우드에서 추론당 최소 비용
    - LLM 및 확산 모델에서 추론당 최소 비용으로 고성능 제공
  - LLM 실무자는 AWS Neuron SDK를 사용하여 고성능 훈련을 수행하고, Trainium 및 Inferentia에서는 각각 고성능과 짧은 지연 시간 추론을 수행할 수 있습니다. 
